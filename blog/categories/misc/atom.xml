<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: misc | Will You Harry Me]]></title>
  <link href="http://harry.me/blog/categories/misc/atom.xml" rel="self"/>
  <link href="http://harry.me/"/>
  <updated>2014-12-27T16:31:35-05:00</updated>
  <id>http://harry.me/</id>
  <author>
    <name><![CDATA[Harry Brundage]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On Software Development Metrics]]></title>
    <link href="http://harry.me/blog/2014/08/16/on-software-development-metrics/"/>
    <updated>2014-08-16T12:57:00-04:00</updated>
    <id>http://harry.me/blog/2014/08/16/on-software-development-metrics</id>
    <content type="html"><![CDATA[<p>In which I try to justify data driven software development, just not for performance management.</p>

<!--more-->


<p>Shopify, where I work, has a business unit whose performance measurement and goals are all completely data driven. We know with a good degree of accuracy if the group is hitting its goals, we know exactly who in the group is excelling and who could use some help, and we know exactly how happy the clients of the group are. We sign contracts with business partners guaranteeing this group's performance because we are confident in it, and the data powering these measurements. These measurements are quantifiable data, which is amazing because we can slice and dice it to learn more about the nature of the group's performance and goals. We can ask valuable operational questions like "when during the week does the work load mean we need to schedule more people", or "how many people do we need to hire next quarter to keep our customers happy". We can ask valuable strategic questions as well, like "does this change to the product affect outcomes", or "should we switch everyone over to this new potentially more productive tool". Hard data powers better insight.</p>

<p>This group is, unfortunately, not comprised of software developers like me, but of sales and support staff at Shopify. They're measured using metrics: how many people did they talk to today, how long did they talk to each of them for, which of those people said the experience was good or bad, etc, which powers the above decisions. For all the concerns the support group has down pat we developers have little to no analog. We have no objective benchmark which tells us if we are meeting all our obligations, we have no objective measure of individual performance for accolades or accusations, and we only have murky, through the grapevine indications of how satisfied our development group's clients are. We can't really predict demand for developers with anything other than a loose survey of the team leads, and we struggle to run experiments concerning techniques or tooling using data to actually make it a bona fide experiment. This upsets me, because I believe that this lack of data inhibits effective decision making for my business group. I'd really like to be able to run experiments, or to give long term hiring estimates to finance, or to understand internal customer satisfaction with our deliverables, but we just don't have the data to power these insights.</p>

<p>So, how could we measure developers and the software development process to try to drive answers to the above questions using data? Well, the industry consensus, and the ideology inside Shopify, is that you can't.</p>

<p>A mantra often repeated inside Shopify is "if you want a number to go up, put it on a dashboard", and I've found this to be true many times over. A metric gives us a clear goal and a clear report on our progress towards it, so we start getting rewarding feedback cycles as we accomplish things that push that metric in the right direction. We make changes to the product or the code, we see the metric on the dashboard change for the better, and we get our dopamine or our promotion or whatever. This said, every metric has a dual nature: it encourages those who care about it to figure out how to push it in the right direction, but at the cost of that metric potentially forcing people to care about the wrong thing. For the metric to encourage the correct behaviour, it must accurately capture the true goals of the business. If it doesn't, as soon as anyone or anything's performance is tied to that metric, they are likely going to start working towards improving it above serving the underlying business goals. Aligning people with a metric only serves the business if the metric captures the business' values completely, lest the metric be gamed.</p>

<p>Take, for example "average customer satisfaction as measured by a short survey". If we decide to reward service staff based on this metric (among others), we will likely have happier customers, because our service staff is encouraged to satisfy customers. This aligns with the business goal of making more money by keeping customers around, so it is a good metric to stick on dashboards.</p>

<p>Take, for a counter example, a metric like "lines of code added or removed this week" as a way to compare developers. If we started paying developers on a per line basis, we'd start seeing people making gigantic, overly verbose pull requests full of needless code and comments, because they'd get paid more! This does not align with the business goal of developing product faster than our competitors, because developers will be busy writing useless comments and hard-to-maintain complex code. This is thus a bad metric, and not suitable for dashboarding or performance management.</p>

<p>This conundrum of capturing the business goals with a metric is the oft-touted reason that software developers often go without quantitative measurement, at least in a performance management context. No one has really thought of a good metric or combination thereof that really encapsulates all the competing goals during software development. The most frequently pondered metrics are things like lines of code added or removed, automated code complexity reporting, test coverage, test run time and run frequency, code churn / change frequency, or defect discovery or fix rate, which are all really elementary, shortsighted observations about the happenings with the code. These metrics don't bake in much understanding of true causality, long term maintainability, performance, security, among many other competing concerns good software developers spend time caring about.</p>

<p>The fact that we can't come up with a suitable performance measurement scheme does not mean we shouldn't measure the process though. Lines of code added or removed this period isn't suitable for a feedback system in a dashboard, but it is still an interesting measurement to have on a report. If it grows like crazy all of a sudden, don't you think it is worth investigating why? I've only ever heard of people not caring about this metric, or taking a casual glance at it in Github Pulse, but it really is correlated with important things. If a new developer starts and the rate spikes, that developer could likely use some feedback about simplicity and brevity. If it doesn't change at all when a developer leaves, perhaps it is a good thing that developer has left, as the absence of their contributions should have at least been felt in the metric. The data that we do have is not useful for holistically measuring developers for performance review purposes, but it is useful for other insight. We correctly hesitate to practice <em>data driven</em> decision making using metrics like lines of code, but we forget that you can still make <em>data informed</em> decisions using these metrics as indicators.</p>

<p>For more examples: if test coverage plummets over the course of a few weeks, I'd love to have a dashboard which tells me where and who authored the new, uncovered code. If one particular area of the code is changing over and over, it's likely a good candidate for the next refactor to try to make this change easier. If we had a report about the most frequently failed tests on local developer's full suite runs, we should probably look at the top failures to see if they are easy to understand or perhaps overly brittle.</p>

<p>The benefits of data warehousing apply just as well: by mixing and matching this data with itself, and other data from the organization, we are able to do incredible stuff we couldn't do before. We could join the lines added / removed history with the list of security incidents to see how old previously insecure code was, and then prompt an audit of code in the same age range to spot security issues before anyone nefarious beats us to it. We could correlate areas of code change with the aforementioned customer satisfaction surveys to see if we can tease out previously unknown relationships between changes to the product and changes in how customers perceive it. We could build data products for ourselves as well: we could make a bot which comments on Github when someone changes a particularly defective piece of code warning them to be extra careful, or we could optimize the order our tests run in so that those most likely to fail run first to give us fast feedback. So far at Shopify we've had success reporting on which sections of our codebase need the most love by counting Github issues opened and closed segmented by label, as well as reporting on production exceptions and which areas they have occurred in.</p>

<p>In summary, don't let the fear of imperfect metrics for performance management stop you from gathering data, and doing some analysis on the software development process. Data driven organizations are more successful, and software development should be no exception.</p>

<h3>Further reading:</h3>

<ul>
<li>Measuring Performance Management in Organizations book by Robert D. Austin: <a href="http://www.amazon.com/gp/product/0932633366">http://www.amazon.com/gp/product/0932633366</a></li>
<li>A Stack Overflow discussion on measuring developer performance using data: <a href="http://pm.stackexchange.com/questions/5289/how-do-i-measure-employee-software-developer-performance-based-on-bugs-created">http://pm.stackexchange.com/questions/5289/how-do-i-measure-employee-software-developer-performance-based-on-bugs-created</a></li>
<li>IBM whitepaper on developer performance measurement using data: <a href="https://jazz.net/library/content/articles/insight/performance-management.pdf">https://jazz.net/library/content/articles/insight/performance-management.pdf</a> , and an article on the implementation of this: <a href="http://www.networkworld.com/article/2182958/software/how-ibm-started-grading-its-developers--productivity.html">http://www.networkworld.com/article/2182958/software/how-ibm-started-grading-its-developers--productivity.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I'd like to have an argument: A primer on consensus]]></title>
    <link href="http://harry.me/blog/2013/07/07/id-like-to-have-an-argument-a-primer-on-consensus/"/>
    <updated>2013-07-07T14:32:00-04:00</updated>
    <id>http://harry.me/blog/2013/07/07/id-like-to-have-an-argument-a-primer-on-consensus</id>
    <content type="html"><![CDATA[<p>If you are, say, a piece of e-commerce software, and, say, you want a record of when your customers buy stuff, how might you ensure that you get a record of each transaction every single time one happens? Money is changing hands, and people aren't very fond of losing money without any gain in return, so having a correct ledger for transactions sure is important.</p>

<!--more-->


<p>You certainly can't just leave this record on one computer, since that computer's disk might die and leave you with none of your data. You could put it on two computers so that if one died you'd still have the record on the other computer, but you'd have to make sure that you write the information to both computers every time. Worse is that you must make sure that your cherished record gets written to both computers before you move on and accept more transactions, because if it doesn't assuredly make it to both places, there's a small chance you might only have one copy, and thus again risk losing that data.</p>

<p>If, say, your data set grew to be so important that simple dual redundancy was inadequate, you could network some computers who would all be responsible for storing the data. Now that this has happened you are beginning to have an interesting problem: how does your system behave when one of these computers fails? Ideally, if only one of ten of the machines gets its power cord tripped over, you should still be able to add more stuff to the other computers. After all, the more computers we add to increase redundancy, the more likely any failure at all is to occur, since we now have ten things that can fail instead of just one. We still want to make sure that when we write some data to this cluster, it is assuredly written to some bunch of boxes, but ideally it doesn't need to be all ten so that the system can sustain inevitable failures.</p>

<h3>This isn't an argument, it's just contradiction!</h3>

<p>A possible strategy would be to designate one computer as the "master", whose responsibilities would be to manage all the incoming write requests from clients of the system by doling them out to the other computers which it knows are online. Designating a master sounds good since we now have one computer who can decide if the system is ready to accept writes. This is to say that if enough computers fail, our beloved transaction ledger thing should enter an "unwritable" state, where no transactions can occur because we can't safely store them. For this transaction log, we'd rather go down than lose data, again because people sure do love their money.</p>

<p>So, going with this strategy for a moment longer, we could program our master node to watch for node deaths, and decide if there is still enough online to continue accepting writes. There is one major glaring problem however: the master itself might fail. We'd need a new master, and lickity split. Then you might think, well, I'll just have some other computer detect that the master computer has failed, and designate another one as the master! Easy peasy.</p>

<p>As simple as that you have stumbled upon a tough computer science problem. Whichever computers remain after a master failure need to somehow arrive at an agreement on who is going to be the next master. If all the computers leapt up and declared themselves the master, we could start having two different data sets, where depending on who you ask the same person has different amounts of money! If no computers declare themselves the master, the system stops working, and no one can buy stuff, which is also less than ideal. The process these computers should follow to designate one and only one new master is called reaching consensus.</p>

<p>The consensus problem is one of the quintessential building blocks of distributed systems, and seems to be regarded as one of the tougher ones from both a conceptual and software engineering point of view. Depending on what subset of the problems you look at, the aim is to define a rigorous process for submitting a value to a cluster of machines who will try reach consensus in the face of the expected failures, or unexpected ones like buggy software, or even goodness gracious holy macaroni <em>malicious agents</em> participating in the cluster. The cluster can agree to not accept a new value when one is submitted, or it can take a significant amount of time to accept it, but the key is that by the end of the process, the cluster "agrees" on what the "true" value is. This true value could be the one the cluster held before anything happened, or the newly submitted value, but the idea is that there is only one. The "true" value here is a convenient yet misleading metaphor, since again, depending on who or how many people you ask, the answer is different. That said, the role of a consensus algorithm is to define both how to submit a new value to the system, and also how to retrieve the "true" value the system has adopted. A handy definition of the "true" value read algorithm is just to ask everyone and see what value the majority of the cluster thinks the value is.</p>

<p>The reasons this problem is challenging arise from the simple fact that both processes and humans are unreliable. Disks fail, cords get unplugged, engineers write bugs, and yet all the while we still want to buy stuff. It wouldn't be too tough to write a goofy consensus algorithm I shall enjoy titling "dunnolol" which just rejects any new incoming values in the event of any of these failures. Due to these failures' inevitability "dunnolol", despite being simple, is relatively useless. The consensus problem holds us engineers to a higher standard of coming up with a way for a cluster of processes with some errors to remain resilient and still accept new values for data.</p>

<h3>Argument is an intellectual process</h3>

<p>Consensus problem solvers enjoy a number of horrid subproblems stemming from the fact that they must admit that there is such a thing as time. Many clients might try to propose a new value to the system around the same time, so problem solvers have to decide if they are going to impose an ordering on the operations the system takes. Messages between processes might arrive slowly, or even out of order as well, which means state has to be very carefully tracked by all actors in the show. A correct implementation of a solution to the problem must guarantee that one and only one value is agreed upon as the true value by the system at one instant. This means it must be completely resilient to conflicting clients proposing conflicting values, and bake in some sort of prevention of different factions of the system trying to pick one of the clients as the correct one.</p>

<p>All this boils <del>down</del> over into a few decades of research. As best I can tell, the state of the art consensus algorithm is one called <a href="http://en.wikipedia.org/wiki/Paxos_(computer_science">Paxos</a>, so if you are looking to see how things relying on consensus are actually built, I'd say start there. Interestingly very recently a new consensus algorithm has risen to prominence in the zeitgeist: <a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf">Raft</a>. Raft interests me because it's been designed for understandability as well as correctness, so it may be worth investigating as well. There's also a number of resources describing concrete implementations of Paxos and the myriad of challenges associated with it which are simultaneously horrifying and interesting.</p>

<h3>More resources:</h3>

<ul>
<li>Paxos author's list of papers: <a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html">http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html</a></li>
<li>Paxos author's simplest explanation of Paxos: <a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf</a></li>
<li>Seminal paper on Raft: <a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf">https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf</a></li>
<li>Raft's parent project, RAMCloud: <a href="https://ramcloud.stanford.edu/wiki/display/ramcloud/RAMCloud">https://ramcloud.stanford.edu/wiki/display/ramcloud/RAMCloud</a></li>
<li>Google's report on implementing Paxos: <a href="http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf">http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf</a></li>
<li>Microsoft's Will Portnoy's blog on implementing Paxos: <a href="http://blog.willportnoy.com/2012/06/lessons-learned-from-paxos.html">http://blog.willportnoy.com/2012/06/lessons-learned-from-paxos.html</a></li>
<li>Monty Python's "Argument Clinic": <a href="http://www.youtube.com/watch?v=kQFKtI6gn9Y">http://www.youtube.com/watch?v=kQFKtI6gn9Y</a></li>
</ul>


<p> Thanks to <a href="https://twitter.com/camilolopez">@camilo</a> and <a href="https://twitter.com/dellsystem">@dellsystem</a> for helping edit.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reinvigoration: Really just a new theme]]></title>
    <link href="http://harry.me/blog/2013/05/12/reinvigoration-really-just-a-new-theme/"/>
    <updated>2013-05-12T12:41:00-04:00</updated>
    <id>http://harry.me/blog/2013/05/12/reinvigoration-really-just-a-new-theme</id>
    <content type="html"><![CDATA[<p>Yep. That's all. This is a test post, and secretly an attempt at making this blog look a bit more loved, whoopee!</p>
]]></content>
  </entry>
  
</feed>
